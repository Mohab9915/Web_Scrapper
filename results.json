[
  {
    "documentation": "The markdown content is extracted from multiple web pages. Each page's content is concatenated, separated by a horizontal rule (`<hr class='page-separator'>`). The content includes various markdown elements like headings, paragraphs, code blocks (with language identifiers where provided, e.g., python, json, graphql, bash, makefile, swift, csharp, ini, sql, cpp), lists, and links, preserving the original structure as much as possible. This format is suitable for training language models, as it provides structured textual data. Ensure your LLM tokenizer and training pipeline can handle mixed content types and the page separator.",
    "markdownContent": "[unclecode/crawl4ai 43.9k 4.1k](https://github.com/unclecode/crawl4ai)\n\n# Getting Started with Crawl4AI\n\nWelcome to **Crawl4AI**, an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, you‚Äôll:\n\n1. Run your **first crawl** using minimal configuration.\n2. Generate **Markdown** output (and learn how it‚Äôs influenced by content filters).\n3. Experiment with a simple **CSS-based extraction** strategy.\n4. See a glimpse of **LLM-based extraction** (including open-source and closed-source model options).\n5. Crawl a **dynamic** page that loads content via JavaScript.\n\n* * *\n\n## 1\\. Introduction\n\nCrawl4AI provides:\n\n- An asynchronous crawler, **`AsyncWebCrawler`**.\n- Configurable browser and run settings via **`BrowserConfig`** and **`CrawlerRunConfig`**.\n- Automatic HTML-to-Markdown conversion via **`DefaultMarkdownGenerator`** (supports optional filters).\n- Multiple extraction strategies (LLM-based or ‚Äútraditional‚Äù CSS/XPath-based).\n\nBy the end of this guide, you‚Äôll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses ‚ÄúLoad More‚Äù buttons or JavaScript updates.\n\n* * *\n\n## 2\\. Your First Crawl\n\nHere‚Äôs a minimal Python script that creates an **`AsyncWebCrawler`**, fetches a webpage, and prints the first 300 characters of its Markdown output:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\")\n        print(result.markdown[:300])  # Print first 300 chars\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**What‚Äôs happening?**\n\\- **`AsyncWebCrawler`** launches a headless browser (Chromium by default).\n\\- It fetches `https://example.com`.\n\\- Crawl4AI automatically converts the HTML into Markdown.\n\nYou now have a simple, working crawl!\n\n* * *\n\n## 3\\. Basic Configuration (Light Introduction)\n\nCrawl4AI‚Äôs crawler can be heavily customized using two main classes:\n\n1.\\u2000**`BrowserConfig`**: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.).\n\n2.\\u2000**`CrawlerRunConfig`**: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.).\n\nBelow is an example with minimal usage:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    browser_conf = BrowserConfig(headless=True)  # or False to see the browser\n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler(config=browser_conf) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_conf\n        )\n        print(result.markdown)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n> IMPORTANT: By default cache mode is set to `CacheMode.ENABLED`. So to have fresh content, you need to set it to `CacheMode.BYPASS`\n\nWe‚Äôll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling.\n\n* * *\n\n## 4\\. Generating Markdown Output\n\nBy default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a **markdown generator** or **content filter**.\n\n- **`result.markdown`**:\n\nThe direct HTML-to-Markdown conversion.\n- **`result.markdown.fit_markdown`**:\n\nThe same content after applying any configured **content filter** (e.g., `PruningContentFilter`).\n\n### Example: Using a Filter with `DefaultMarkdownGenerator`\n\n```python\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\")\n)\n\nconfig = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    markdown_generator=md_generator\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://news.ycombinator.com\", config=config)\n    print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n    print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\nCopy\n```\n\n**Note**: If you do **not** specify a content filter or markdown generator, you‚Äôll typically see only the raw Markdown. `PruningContentFilter` may adds around `50ms` in processing time. We‚Äôll dive deeper into these strategies in a dedicated **Markdown Generation** tutorial.\n\n* * *\n\n## 5\\. Simple Data Extraction (CSS-based)\n\nCrawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example:\n\n> **New!** Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Generate a schema (one-time cost)\nhtml = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\"\n\n# Using OpenAI (requires API token)\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\")  # Required for OpenAI\n)\n\n# Or using Ollama (open source, no token needed)\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None)  # Not needed for Ollama\n)\n\n# Use the schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(schema)\nCopy\n```\n\nFor a complete guide on schema generation and advanced usage, see [No-LLM Extraction Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/).\n\nHere's a basic extraction example:\n\n```python\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    schema = {\n        \"name\": \"Example Items\",\n        \"baseSelector\": \"div.item\",\n        \"fields\": [\\\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\\\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\\\n        ]\n    }\n\n    raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\"\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"raw://\" + raw_html,\n            config=CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                extraction_strategy=JsonCssExtractionStrategy(schema)\n            )\n        )\n        # The JSON output is stored in 'extracted_content'\n        data = json.loads(result.extracted_content)\n        print(data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Why is this helpful?**\n\\- Great for repetitive page structures (e.g., item listings, articles).\n\\- No AI usage or costs.\n\\- The crawler returns a JSON string you can parse or store.\n\n> Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with `raw://`.\n\n* * *\n\n## 6\\. Simple Data Extraction (LLM-based)\n\nFor more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports **open-source** or **closed-source** providers:\n\n- **Open-Source Models** (e.g., `ollama/llama3.3`, `no_token`)\n- **OpenAI Models** (e.g., `openai/gpt-4`, requires `api_token`)\n- Or any provider supported by the underlying library\n\nBelow is an example using **open-source** style (no token) and closed-source:\n\n```python\nimport os\nimport json\nimport asyncio\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(\n        ..., description=\"Fee for output token for the OpenAI model.\"\n    )\n\nasync def extract_structured_data_using_llm(\n    provider: str, api_token: str = None, extra_headers: Dict[str, str] = None\n):\n    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n\n    if api_token is None and provider != \"ollama\":\n        print(f\"API token is required for {provider}. Skipping this example.\")\n        return\n\n    browser_config = BrowserConfig(headless=True)\n\n    extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000}\n    if extra_headers:\n        extra_args[\"extra_headers\"] = extra_headers\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        word_count_threshold=1,\n        page_timeout=80000,\n        extraction_strategy=LLMExtractionStrategy(\n            llm_config = LLMConfig(provider=provider,api_token=api_token),\n            schema=OpenAIModelFee.model_json_schema(),\n            extraction_type=\"schema\",\n            instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens.\n            Do not miss any models in the entire content.\"\"\",\n            extra_args=extra_args,\n        ),\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://openai.com/api/pricing/\", config=crawler_config\n        )\n        print(result.extracted_content)\n\nif __name__ == \"__main__\":\n\n    asyncio.run(\n        extract_structured_data_using_llm(\n            provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\")\n        )\n    )\nCopy\n```\n\n**What‚Äôs happening?**\n\\- We define a Pydantic schema ( `PricingInfo`) describing the fields we want.\n\\- The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON.\n\\- Depending on the **provider** and **api\\_token**, you can use local models or a remote API.\n\n* * *\n\n## 7\\. Multi-URL Concurrency (Preview)\n\nIf you need to crawl multiple URLs in **parallel**, you can use `arun_many()`. By default, Crawl4AI employs a **MemoryAdaptiveDispatcher**, automatically adjusting concurrency based on system resources. Here‚Äôs a quick glimpse:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def quick_parallel_example():\n    urls = [\\\n        \"https://example.com/page1\",\\\n        \"https://example.com/page2\",\\\n        \"https://example.com/page3\"\\\n    ]\n\n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=True  # Enable streaming mode\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        # Stream results as they complete\n        async for result in await crawler.arun_many(urls, config=run_conf):\n            if result.success:\n                print(f\"[OK] {result.url}, length: {len(result.markdown.raw_markdown)}\")\n            else:\n                print(f\"[ERROR] {result.url} => {result.error_message}\")\n\n        # Or get all results at once (default behavior)\n        run_conf = run_conf.clone(stream=False)\n        results = await crawler.arun_many(urls, config=run_conf)\n        for res in results:\n            if res.success:\n                print(f\"[OK] {res.url}, length: {len(res.markdown.raw_markdown)}\")\n            else:\n                print(f\"[ERROR] {res.url} => {res.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(quick_parallel_example())\nCopy\n```\n\nThe example above shows two ways to handle multiple URLs:\n1\\. **Streaming mode** ( `stream=True`): Process results as they become available using `async for`\n2\\. **Batch mode** ( `stream=False`): Wait for all results to complete\n\nFor more advanced concurrency (e.g., a **semaphore-based** approach, **adaptive memory usage throttling**, or customized rate limiting), see [Advanced Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/).\n\n* * *\n\n## 8\\. Dynamic Content Example\n\nSome sites require multiple ‚Äúpage clicks‚Äù or dynamic JavaScript updates. Below is an example showing how to **click** a ‚ÄúNext Page‚Äù button and wait for new commits to load on GitHub, using **`BrowserConfig`** and **`CrawlerRunConfig`**:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    schema = {\n        \"name\": \"KidoCode Courses\",\n        \"baseSelector\": \"section.charge-methodology .w-tab-content > div\",\n        \"fields\": [\\\n            {\\\n                \"name\": \"section_title\",\\\n                \"selector\": \"h3.heading-50\",\\\n                \"type\": \"text\",\n            },\\\n            {\\\n                \"name\": \"section_description\",\\\n                \"selector\": \".charge-content\",\\\n                \"type\": \"text\",\n            },\\\n            {\\\n                \"name\": \"course_name\",\\\n                \"selector\": \".text-block-93\",\\\n                \"type\": \"text\",\n            },\\\n            {\\\n                \"name\": \"course_description\",\\\n                \"selector\": \".course-content-text\",\\\n                \"type\": \"text\",\n            },\\\n            {\\\n                \"name\": \"course_icon\",\\\n                \"selector\": \".image-92\",\\\n                \"type\": \"attribute\",\\\n                \"attribute\": \"src\",\n            },\\\n        ],\n    }\n\n    browser_config = BrowserConfig(headless=True, java_script_enabled=True)\n\n    js_click_tabs = \"\"\"\n    (async () => {\n        const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");\n        for(let tab of tabs) {\n            tab.scrollIntoView();\n            tab.click();\n            await new Promise(r => setTimeout(r, 500));\n        }\n    })();\n    \"\"\"\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        js_code=[js_click_tabs],\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config\n        )\n\n        companies = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(companies)} companies\")\n        print(json.dumps(companies[0], indent=2))\n\nasync def main():\n    await extract_structured_data_using_css_extractor()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Key Points**:\n\n- **`BrowserConfig(headless=False)`**: We want to watch it click ‚ÄúNext Page.‚Äù\n- **`CrawlerRunConfig(...)`**: We specify the extraction strategy, pass `session_id` to reuse the same page.\n- **`js_code`** and **`wait_for`** are used for subsequent pages ( `page > 0`) to click the ‚ÄúNext‚Äù button and wait for new commits to load.\n- **`js_only=True`** indicates we‚Äôre not re-navigating but continuing the existing session.\n- Finally, we call `kill_session()` to clean up the page and browser session.\n\n* * *\n\n## 9\\. Next Steps\n\nCongratulations! You have:\n\n1. Performed a basic crawl and printed Markdown.\n2. Used **content filters** with a markdown generator.\n3. Extracted JSON via **CSS** or **LLM** strategies.\n4. Handled **dynamic** pages with JavaScript triggers.\n\nIf you‚Äôre ready for more, check out:\n\n- **Installation**: A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies.\n- **Hooks & Auth**: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc.\n- **Deployment**: Explore ephemeral testing in Docker or plan for the upcoming stable Docker release.\n- **Browser Management**: Delve into user simulation, stealth modes, and concurrency best practices.\n\nCrawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling!\n\n* * *\n\n>\nFeedback\n\n[Ask AI](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n\n<hr class='page-separator'>\n\n[unclecode/crawl4ai 43.9k 4.1k](https://github.com/unclecode/crawl4ai)\n\n# üöÄü§ñ Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper\n\n[![unclecode%2Fcrawl4ai | Trendshift](https://trendshift.io/api/badge/repositories/11716)](https://trendshift.io/repositories/11716)\n\n[![GitHub Stars](https://img.shields.io/github/stars/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/stargazers)[![GitHub Forks](https://img.shields.io/github/forks/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/network/members)[![PyPI version](https://badge.fury.io/py/crawl4ai.svg)](https://badge.fury.io/py/crawl4ai)\n\n[![Python Version](https://img.shields.io/pypi/pyversions/crawl4ai)](https://pypi.org/project/crawl4ai/)[![Downloads](https://static.pepy.tech/badge/crawl4ai/month)](https://pepy.tech/project/crawl4ai)[![License](https://img.shields.io/github/license/unclecode/crawl4ai)](https://github.com/unclecode/crawl4ai/blob/main/LICENSE)\n\nCrawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, **Crawl4AI** empowers developers with unmatched speed, precision, and deployment ease.\n\n> **Note**: If you're looking for the old documentation, you can access it [here](https://old.docs.crawl4ai.com/).\n\n## Quick Start\n\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler() as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://crawl4ai.com\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\nCopy\n```\n\n* * *\n\n## Video Tutorial\n\nCrawl4ai Official Tutorial, Full 1hr with Quickstart Examples - YouTube\n\nUnclecode\n\n1.38K subscribers\n\n[Crawl4ai Official Tutorial, Full 1hr with Quickstart Examples](https://www.youtube.com/watch?v=xo3qK6Hg9AA)\n\nUnclecode\n\nSearch\n\nWatch later\n\nShare\n\nCopy link\n\nInfo\n\nShopping\n\nTap to unmute\n\nIf playback doesn't begin shortly, try restarting your device.\n\nMore videos\n\n## More videos\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\n\nCancelConfirm\n\nShare\n\nInclude playlist\n\nAn error occurred while retrieving sharing information. Please try again later.\n\n[Watch on](https://www.youtube.com/watch?t=15&v=xo3qK6Hg9AA&embeds_referring_euri=https%3A%2F%2Fdocs.crawl4ai.com%2F)\n\n0:15\n\n0:15 / 1:02:39\n‚Ä¢Live\n\n‚Ä¢\n\n[Watch on YouTube](https://www.youtube.com/watch?v=xo3qK6Hg9AA \"Watch on YouTube\")\n\n* * *\n\n## What Does Crawl4AI Do?\n\nCrawl4AI is a feature-rich crawler and scraper that aims to:\n\n1.\\u2000**Generate Clean Markdown**: Perfect for RAG pipelines or direct ingestion into LLMs.\n\n2.\\u2000**Structured Extraction**: Parse repeated patterns with CSS, XPath, or LLM-based extraction.\n\n3.\\u2000**Advanced Browser Control**: Hooks, proxies, stealth modes, session re-use‚Äîfine-grained control.\n\n4.\\u2000**High Performance**: Parallel crawling, chunk-based extraction, real-time use cases.\n\n5.\\u2000**Open Source**: No forced API keys, no paywalls‚Äîeveryone can access their data.\n\n**Core Philosophies**:\n\\- **Democratize Data**: Free to use, transparent, and highly configurable.\n\n\\- **LLM Friendly**: Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it.\n\n* * *\n\n## Documentation Structure\n\nTo help you get started, we‚Äôve organized our docs into clear sections:\n\n- **Setup & Installation**\n\nBasic instructions to install Crawl4AI via pip or Docker.\n- **Quick Start**\n\nA hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction.\n- **Core**\n\nDeeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching.\n- **Advanced**\n\nExplore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more.\n- **Extraction**\n\nDetailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches.\n- **API Reference**\n\nFind the technical specifics of each class and method, including `AsyncWebCrawler`, `arun()`, and `CrawlResult`.\n\nThroughout these sections, you‚Äôll find code samples you can **copy-paste** into your environment. If something is missing or unclear, raise an issue or PR.\n\n* * *\n\n## How You Can Support\n\n- **Star & Fork**: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features.\n- **File Issues**: Encounter a bug or missing feature? Let us know by filing an issue, so we can improve.\n- **Pull Requests**: Whether it‚Äôs a small fix, a big feature, or better docs‚Äîcontributions are always welcome.\n- **Join Discord**: Come chat about web scraping, crawling tips, or AI workflows with the community.\n- **Spread the Word**: Mention Crawl4AI in your blog posts, talks, or on social media.\n\n**Our mission**: to empower everyone‚Äîstudents, researchers, entrepreneurs, data scientists‚Äîto access, parse, and shape the world‚Äôs data with speed, cost-efficiency, and creative freedom.\n\n* * *\n\n## Quick Links\n\n- **[GitHub Repo](https://github.com/unclecode/crawl4ai)**\n- **[Installation Guide](https://docs.crawl4ai.com/core/installation/)**\n- **[Quick Start](https://docs.crawl4ai.com/core/quickstart/)**\n- **[API Reference](https://docs.crawl4ai.com/api/async-webcrawler/)**\n- **[Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)**\n\nThank you for joining me on this journey. Let‚Äôs keep building an **open, democratic** approach to data extraction and AI together.\n\nHappy Crawling!\n\n‚Äî _Unclecode, Founder & Maintainer of Crawl4AI_\n\n* * *\n\n>\nFeedback\n\n[Ask AI](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n\n<hr class='page-separator'>\n\n[unclecode/crawl4ai 43.9k 4.1k](https://github.com/unclecode/crawl4ai)\n\n# Markdown Generation Basics\n\nOne of Crawl4AI‚Äôs core features is generating **clean, structured markdown** from web pages. Originally built to solve the problem of extracting only the ‚Äúactual‚Äù content and discarding boilerplate or noise, Crawl4AI‚Äôs markdown system remains one of its biggest draws for AI workflows.\n\nIn this tutorial, you‚Äôll learn:\n\n1. How to configure the **Default Markdown Generator**\n2. How **content filters** (BM25 or Pruning) help you refine markdown and discard junk\n3. The difference between raw markdown ( `result.markdown`) and filtered markdown ( `fit_markdown`)\n\n> **Prerequisites**\n>\n> \\- You‚Äôve completed or read [AsyncWebCrawler Basics](https://docs.crawl4ai.com/core/simple-crawling/) to understand how to run a simple crawl.\n>\n> \\- You know how to configure `CrawlerRunConfig`.\n\n* * *\n\n## 1\\. Quick Example\n\nHere‚Äôs a minimal code snippet that uses the **DefaultMarkdownGenerator** with no additional filtering:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    config = CrawlerRunConfig(\n        markdown_generator=DefaultMarkdownGenerator()\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n\n        if result.success:\n            print(\"Raw Markdown Output:\\n\")\n            print(result.markdown)  # The unfiltered markdown from the page\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**What‚Äôs happening?**\n\n\\- `CrawlerRunConfig( markdown_generator = DefaultMarkdownGenerator() )` instructs Crawl4AI to convert the final HTML into markdown at the end of each crawl.\n\n\\- The resulting markdown is accessible via `result.markdown`.\n\n* * *\n\n## 2\\. How Markdown Generation Works\n\n### 2.1 HTML-to-Text Conversion (Forked & Modified)\n\nUnder the hood, **DefaultMarkdownGenerator** uses a specialized HTML-to-text approach that:\n\n- Preserves headings, code blocks, bullet points, etc.\n- Removes extraneous tags (scripts, styles) that don‚Äôt add meaningful content.\n- Can optionally generate references for links or skip them altogether.\n\nA set of **options** (passed as a dict) allows you to customize precisely how HTML converts to markdown. These map to standard html2text-like configuration plus your own enhancements (e.g., ignoring internal links, preserving certain tags verbatim, or adjusting line widths).\n\n### 2.2 Link Citations & References\n\nBy default, the generator can convert `<a href=\"...\">` elements into `[text][1]` citations, then place the actual links at the bottom of the document. This is handy for research workflows that demand references in a structured manner.\n\n### 2.3 Optional Content Filters\n\nBefore or after the HTML-to-Markdown step, you can apply a **content filter** (like BM25 or Pruning) to reduce noise and produce a ‚Äúfit\\_markdown‚Äù‚Äîa heavily pruned version focusing on the page‚Äôs main text. We‚Äôll cover these filters shortly.\n\n* * *\n\n## 3\\. Configuring the Default Markdown Generator\n\nYou can tweak the output by passing an `options` dict to `DefaultMarkdownGenerator`. For example:\n\n```python\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Example: ignore all links, don't escape HTML, and wrap text at 80 characters\n    md_generator = DefaultMarkdownGenerator(\n        options={\n            \"ignore_links\": True,\n            \"escape_html\": False,\n            \"body_width\": 80\n        }\n    )\n\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/docs\", config=config)\n        if result.success:\n            print(\"Markdown:\\n\", result.markdown[:500])  # Just a snippet\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\nCopy\n```\n\nSome commonly used `options`:\n\n- **`ignore_links`** (bool): Whether to remove all hyperlinks in the final markdown.\n- **`ignore_images`** (bool): Remove all `![image]()` references.\n- **`escape_html`** (bool): Turn HTML entities into text (default is often `True`).\n- **`body_width`** (int): Wrap text at N characters. `0` or `None` means no wrapping.\n- **`skip_internal_links`** (bool): If `True`, omit `#localAnchors` or internal links referencing the same page.\n- **`include_sup_sub`** (bool): Attempt to handle `<sup>` / `<sub>` in a more readable way.\n\n## 4\\. Selecting the HTML Source for Markdown Generation\n\nThe `content_source` parameter allows you to control which HTML content is used as input for markdown generation. This gives you flexibility in how the HTML is processed before conversion to markdown.\n\n```python\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Option 1: Use the raw HTML directly from the webpage (before any processing)\n    raw_md_generator = DefaultMarkdownGenerator(\n        content_source=\"raw_html\",\n        options={\"ignore_links\": True}\n    )\n\n    # Option 2: Use the cleaned HTML (after scraping strategy processing - default)\n    cleaned_md_generator = DefaultMarkdownGenerator(\n        content_source=\"cleaned_html\",  # This is the default\n        options={\"ignore_links\": True}\n    )\n\n    # Option 3: Use preprocessed HTML optimized for schema extraction\n    fit_md_generator = DefaultMarkdownGenerator(\n        content_source=\"fit_html\",\n        options={\"ignore_links\": True}\n    )\n\n    # Use one of the generators in your crawler config\n    config = CrawlerRunConfig(\n        markdown_generator=raw_md_generator  # Try each of the generators\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        if result.success:\n            print(\"Markdown:\\n\", result.markdown.raw_markdown[:500])\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\nCopy\n```\n\n### HTML Source Options\n\n- **`\"cleaned_html\"`** (default): Uses the HTML after it has been processed by the scraping strategy. This HTML is typically cleaner and more focused on content, with some boilerplate removed.\n\n- **`\"raw_html\"`**: Uses the original HTML directly from the webpage, before any cleaning or processing. This preserves more of the original content, but may include navigation bars, ads, footers, and other elements that might not be relevant to the main content.\n\n- **`\"fit_html\"`**: Uses HTML preprocessed for schema extraction. This HTML is optimized for structured data extraction and may have certain elements simplified or removed.\n\n### When to Use Each Option\n\n- Use **`\"cleaned_html\"`** (default) for most cases where you want a balance of content preservation and noise removal.\n- Use **`\"raw_html\"`** when you need to preserve all original content, or when the cleaning process is removing content you actually want to keep.\n- Use **`\"fit_html\"`** when working with structured data or when you need HTML that's optimized for schema extraction.\n\n* * *\n\n## 5\\. Content Filters\n\n**Content filters** selectively remove or rank sections of text before turning them into Markdown. This is especially helpful if your page has ads, nav bars, or other clutter you don‚Äôt want.\n\n### 5.1 BM25ContentFilter\n\nIf you have a **search query**, BM25 is a good choice:\n\n```python\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\nfrom crawl4ai import CrawlerRunConfig\n\nbm25_filter = BM25ContentFilter(\n    user_query=\"machine learning\",\n    bm25_threshold=1.2,\n    use_stemming=True\n)\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=bm25_filter,\n    options={\"ignore_links\": True}\n)\n\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\nCopy\n```\n\n- **`user_query`**: The term you want to focus on. BM25 tries to keep only content blocks relevant to that query.\n- **`bm25_threshold`**: Raise it to keep fewer blocks; lower it to keep more.\n- **`use_stemming`**: If `True`, variations of words match (e.g., ‚Äúlearn,‚Äù ‚Äúlearning,‚Äù ‚Äúlearnt‚Äù).\n\n**No query provided?** BM25 tries to glean a context from page metadata, or you can simply treat it as a scorched-earth approach that discards text with low generic score. Realistically, you want to supply a query for best results.\n\n### 5.2 PruningContentFilter\n\nIf you **don‚Äôt** have a specific query, or if you just want a robust ‚Äújunk remover,‚Äù use `PruningContentFilter`. It analyzes text density, link density, HTML structure, and known patterns (like ‚Äúnav,‚Äù ‚Äúfooter‚Äù) to systematically prune extraneous or repetitive sections.\n\n```cpp\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\n\nprune_filter = PruningContentFilter(\n    threshold=0.5,\n    threshold_type=\"fixed\",  # or \"dynamic\"\n    min_word_threshold=50\n)\nCopy\n```\n\n- **`threshold`**: Score boundary. Blocks below this score get removed.\n- **`threshold_type`**:\n  - `\"fixed\"`: Straight comparison ( `score >= threshold` keeps the block).\n  - `\"dynamic\"`: The filter adjusts threshold in a data-driven manner.\n- **`min_word_threshold`**: Discard blocks under N words as likely too short or unhelpful.\n\n**When to Use PruningContentFilter**\n\n\\- You want a broad cleanup without a user query.\n\n\\- The page has lots of repeated sidebars, footers, or disclaimers that hamper text extraction.\n\n### 5.3 LLMContentFilter\n\nFor intelligent content filtering and high-quality markdown generation, you can use the **LLMContentFilter**. This filter leverages LLMs to generate relevant markdown while preserving the original content's meaning and structure:\n\n```python\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig\nfrom crawl4ai.content_filter_strategy import LLMContentFilter\n\nasync def main():\n    # Initialize LLM filter with specific instruction\n    filter = LLMContentFilter(\n        llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-api-token\"), #or use environment variable\n        instruction=\"\"\"\n        Focus on extracting the core educational content.\n        Include:\n        - Key concepts and explanations\n        - Important code examples\n        - Essential technical details\n        Exclude:\n        - Navigation elements\n        - Sidebars\n        - Footer content\n        Format the output as clean markdown with proper code blocks and headers.\n        \"\"\",\n        chunk_token_threshold=4096,  # Adjust based on your needs\n        verbose=True\n    )\n\n    config = CrawlerRunConfig(\n        content_filter=filter\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        print(result.markdown.fit_markdown)  # Filtered markdown content\nCopy\n```\n\n**Key Features:**\n\\- **Intelligent Filtering**: Uses LLMs to understand and extract relevant content while maintaining context\n\\- **Customizable Instructions**: Tailor the filtering process with specific instructions\n\\- **Chunk Processing**: Handles large documents by processing them in chunks (controlled by `chunk_token_threshold`)\n\\- **Parallel Processing**: For better performance, use smaller `chunk_token_threshold` (e.g., 2048 or 4096) to enable parallel processing of content chunks\n\n**Two Common Use Cases:**\n\n1. **Exact Content Preservation**:\n\n```python\nfilter = LLMContentFilter(\n       instruction=\"\"\"\n       Extract the main educational content while preserving its original wording and substance completely.\n    1. Maintain the exact language and terminology\n    2. Keep all technical explanations and examples intact\n    3. Preserve the original flow and structure\n    4. Remove only clearly irrelevant elements like navigation menus and ads\n    \"\"\",\n    chunk_token_threshold=4096\n)\nCopy\n```\n\n2. **Focused Content Extraction**:\n\n```python\nfilter = LLMContentFilter(\n       instruction=\"\"\"\n       Focus on extracting specific types of content:\n    - Technical documentation\n    - Code examples\n    - API references\n    Reformat the content into clear, well-structured markdown\n    \"\"\",\n    chunk_token_threshold=4096\n)\nCopy\n```\n\n> **Performance Tip**: Set a smaller `chunk_token_threshold` (e.g., 2048 or 4096) to enable parallel processing of content chunks. The default value is infinity, which processes the entire content as a single chunk.\n\n* * *\n\n## 6\\. Using Fit Markdown\n\nWhen a content filter is active, the library produces two forms of markdown inside `result.markdown`:\n\n1.\\u2000**`raw_markdown`**: The full unfiltered markdown.\n\n2.\\u2000**`fit_markdown`**: A ‚Äúfit‚Äù version where the filter has removed or trimmed noisy segments.\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\n\nasync def main():\n    config = CrawlerRunConfig(\n        markdown_generator=DefaultMarkdownGenerator(\n            content_filter=PruningContentFilter(threshold=0.6),\n            options={\"ignore_links\": True}\n        )\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://news.example.com/tech\", config=config)\n        if result.success:\n            print(\"Raw markdown:\\n\", result.markdown)\n\n            # If a filter is used, we also have .fit_markdown:\n            md_object = result.markdown  # or your equivalent\n            print(\"Filtered markdown:\\n\", md_object.fit_markdown)\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n* * *\n\n## 7\\. The `MarkdownGenerationResult` Object\n\nIf your library stores detailed markdown output in an object like `MarkdownGenerationResult`, you‚Äôll see fields such as:\n\n- **`raw_markdown`**: The direct HTML-to-markdown transformation (no filtering).\n- **`markdown_with_citations`**: A version that moves links to reference-style footnotes.\n- **`references_markdown`**: A separate string or section containing the gathered references.\n- **`fit_markdown`**: The filtered markdown if you used a content filter.\n- **`fit_html`**: The corresponding HTML snippet used to generate `fit_markdown` (helpful for debugging or advanced usage).\n\n**Example**:\n\n```swift\nmd_obj = result.markdown  # your library‚Äôs naming may vary\nprint(\"RAW:\\n\", md_obj.raw_markdown)\nprint(\"CITED:\\n\", md_obj.markdown_with_citations)\nprint(\"REFERENCES:\\n\", md_obj.references_markdown)\nprint(\"FIT:\\n\", md_obj.fit_markdown)\nCopy\n```\n\n**Why Does This Matter?**\n\n\\- You can supply `raw_markdown` to an LLM if you want the entire text.\n\n\\- Or feed `fit_markdown` into a vector database to reduce token usage.\n\n\\- `references_markdown` can help you keep track of link provenance.\n\n* * *\n\nBelow is a **revised section** under ‚ÄúCombining Filters (BM25 + Pruning)‚Äù that demonstrates how you can run **two** passes of content filtering without re-crawling, by taking the HTML (or text) from a first pass and feeding it into the second filter. It uses real code patterns from the snippet you provided for **BM25ContentFilter**, which directly accepts **HTML** strings (and can also handle plain text with minimal adaptation).\n\n* * *\n\n## 8\\. Combining Filters (BM25 + Pruning) in Two Passes\n\nYou might want to **prune out** noisy boilerplate first (with `PruningContentFilter`), and then **rank what‚Äôs left** against a user query (with `BM25ContentFilter`). You don‚Äôt have to crawl the page twice. Instead:\n\n1.\\u2000**First pass**: Apply `PruningContentFilter` directly to the raw HTML from `result.html` (the crawler‚Äôs downloaded HTML).\n\n2.\\u2000**Second pass**: Take the pruned HTML (or text) from step 1, and feed it into `BM25ContentFilter`, focusing on a user query.\n\n### Two-Pass Example\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter\nfrom bs4 import BeautifulSoup\n\nasync def main():\n    # 1. Crawl with minimal or no markdown generator, just get raw HTML\n    config = CrawlerRunConfig(\n        # If you only want raw HTML, you can skip passing a markdown_generator\n        # or provide one but focus on .html in this example\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/tech-article\", config=config)\n\n        if not result.success or not result.html:\n            print(\"Crawl failed or no HTML content.\")\n            return\n\n        raw_html = result.html\n\n        # 2. First pass: PruningContentFilter on raw HTML\n        pruning_filter = PruningContentFilter(threshold=0.5, min_word_threshold=50)\n\n        # filter_content returns a list of \"text chunks\" or cleaned HTML sections\n        pruned_chunks = pruning_filter.filter_content(raw_html)\n        # This list is basically pruned content blocks, presumably in HTML or text form\n\n        # For demonstration, let's combine these chunks back into a single HTML-like string\n        # or you could do further processing. It's up to your pipeline design.\n        pruned_html = \"\\n\".join(pruned_chunks)\n\n        # 3. Second pass: BM25ContentFilter with a user query\n        bm25_filter = BM25ContentFilter(\n            user_query=\"machine learning\",\n            bm25_threshold=1.2,\n            language=\"english\"\n        )\n\n        # returns a list of text chunks\n        bm25_chunks = bm25_filter.filter_content(pruned_html)\n\n        if not bm25_chunks:\n            print(\"Nothing matched the BM25 query after pruning.\")\n            return\n\n        # 4. Combine or display final results\n        final_text = \"\\n---\\n\".join(bm25_chunks)\n\n        print(\"==== PRUNED OUTPUT (first pass) ====\")\n        print(pruned_html[:500], \"... (truncated)\")  # preview\n\n        print(\"\\n==== BM25 OUTPUT (second pass) ====\")\n        print(final_text[:500], \"... (truncated)\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### What‚Äôs Happening?\n\n1.\\u2000**Raw HTML**: We crawl once and store the raw HTML in `result.html`.\n\n2.\\u2000**PruningContentFilter**: Takes HTML + optional parameters. It extracts blocks of text or partial HTML, removing headings/sections deemed ‚Äúnoise.‚Äù It returns a **list of text chunks**.\n\n3.\\u2000**Combine or Transform**: We join these pruned chunks back into a single HTML-like string. (Alternatively, you could store them in a list for further logic‚Äîwhatever suits your pipeline.)\n\n4.\\u2000**BM25ContentFilter**: We feed the pruned string into `BM25ContentFilter` with a user query. This second pass further narrows the content to chunks relevant to ‚Äúmachine learning.‚Äù\n\n**No Re-Crawling**: We used `raw_html` from the first pass, so there‚Äôs no need to run `arun()` again‚Äî **no second network request**.\n\n### Tips & Variations\n\n- **Plain Text vs. HTML**: If your pruned output is mostly text, BM25 can still handle it; just keep in mind it expects a valid string input. If you supply partial HTML (like `\"<p>some text</p>\"`), it will parse it as HTML.\n- **Chaining in a Single Pipeline**: If your code supports it, you can chain multiple filters automatically. Otherwise, manual two-pass filtering (as shown) is straightforward.\n- **Adjust Thresholds**: If you see too much or too little text in step one, tweak `threshold=0.5` or `min_word_threshold=50`. Similarly, `bm25_threshold=1.2` can be raised/lowered for more or fewer chunks in step two.\n\n### One-Pass Combination?\n\nIf your codebase or pipeline design allows applying multiple filters in one pass, you could do so. But often it‚Äôs simpler‚Äîand more transparent‚Äîto run them sequentially, analyzing each step‚Äôs result.\n\n**Bottom Line**: By **manually chaining** your filtering logic in two passes, you get powerful incremental control over the final content. First, remove ‚Äúglobal‚Äù clutter with Pruning, then refine further with BM25-based query relevance‚Äîwithout incurring a second network crawl.\n\n* * *\n\n## 9\\. Common Pitfalls & Tips\n\n1.\\u2000**No Markdown Output?**\n\n\\- Make sure the crawler actually retrieved HTML. If the site is heavily JS-based, you may need to enable dynamic rendering or wait for elements.\n\n\\- Check if your content filter is too aggressive. Lower thresholds or disable the filter to see if content reappears.\n\n2.\\u2000**Performance Considerations**\n\n\\- Very large pages with multiple filters can be slower. Consider `cache_mode` to avoid re-downloading.\n\n\\- If your final use case is LLM ingestion, consider summarizing further or chunking big texts.\n\n3.\\u2000**Take Advantage of `fit_markdown`**\n\n\\- Great for RAG pipelines, semantic search, or any scenario where extraneous boilerplate is unwanted.\n\n\\- Still verify the textual quality‚Äîsome sites have crucial data in footers or sidebars.\n\n4.\\u2000**Adjusting `html2text` Options**\n\n\\- If you see lots of raw HTML slipping into the text, turn on `escape_html`.\n\n\\- If code blocks look messy, experiment with `mark_code` or `handle_code_in_pre`.\n\n* * *\n\n## 10\\. Summary & Next Steps\n\nIn this **Markdown Generation Basics** tutorial, you learned to:\n\n- Configure the **DefaultMarkdownGenerator** with HTML-to-text options.\n- Select different HTML sources using the `content_source` parameter.\n- Use **BM25ContentFilter** for query-specific extraction or **PruningContentFilter** for general noise removal.\n- Distinguish between raw and filtered markdown ( `fit_markdown`).\n- Leverage the `MarkdownGenerationResult` object to handle different forms of output (citations, references, etc.).\n\nNow you can produce high-quality Markdown from any website, focusing on exactly the content you need‚Äîan essential step for powering AI models, summarization pipelines, or knowledge-base queries.\n\n**Last Updated**: 2025-01-01\n\n* * *\n\n >\n Feedback\n\n[Ask AI](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n\n<hr class='page-separator'>\n\n[unclecode/crawl4ai 43.9k 4.1k](https://github.com/unclecode/crawl4ai)\n\n# Extracting JSON (No LLM)\n\nOne of Crawl4AI's **most powerful** features is extracting **structured JSON** from websites **without** relying on large language models. Crawl4AI offers several strategies for LLM-free extraction:\n\n1. **Schema-based extraction** with CSS or XPath selectors via `JsonCssExtractionStrategy` and `JsonXPathExtractionStrategy`\n2. **Regular expression extraction** with `RegexExtractionStrategy` for fast pattern matching\n\nThese approaches let you extract data instantly‚Äîeven from complex or nested HTML structures‚Äîwithout the cost, latency, or environmental impact of an LLM.\n\n**Why avoid LLM for basic extractions?**\n\n1. **Faster & Cheaper**: No API calls or GPU overhead.\n2. **Lower Carbon Footprint**: LLM inference can be energy-intensive. Pattern-based extraction is practically carbon-free.\n3. **Precise & Repeatable**: CSS/XPath selectors and regex patterns do exactly what you specify. LLM outputs can vary or hallucinate.\n4. **Scales Readily**: For thousands of pages, pattern-based extraction runs quickly and in parallel.\n\nBelow, we'll explore how to craft these schemas and use them with **JsonCssExtractionStrategy** (or **JsonXPathExtractionStrategy** if you prefer XPath). We'll also highlight advanced features like **nested fields** and **base element attributes**.\n\n* * *\n\n## 1\\. Intro to Schema-Based Extraction\n\nA schema defines:\n\n1. A **base selector** that identifies each \"container\" element on the page (e.g., a product row, a blog post card).\n2. **Fields** describing which CSS/XPath selectors to use for each piece of data you want to capture (text, attribute, HTML block, etc.).\n3. **Nested** or **list** types for repeated or hierarchical structures.\n\nFor example, if you have a list of products, each one might have a name, price, reviews, and \"related products.\" This approach is faster and more reliable than an LLM for consistent, structured pages.\n\n* * *\n\n## 2\\. Simple Example: Crypto Prices\n\nLet's begin with a **simple** schema-based extraction using the `JsonCssExtractionStrategy`. Below is a snippet that extracts cryptocurrency prices from a site (similar to the legacy Coinbase example). Notice we **don't** call any LLM:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_crypto_prices():\n    # 1. Define a simple extraction schema\n    schema = {\n        \"name\": \"Crypto Prices\",\n        \"baseSelector\": \"div.crypto-row\",    # Repeated elements\n        \"fields\": [\\\n            {\\\n                \"name\": \"coin_name\",\\\n                \"selector\": \"h2.coin-name\",\\\n                \"type\": \"text\"\\\n            },\\\n            {\\\n                \"name\": \"price\",\\\n                \"selector\": \"span.coin-price\",\\\n                \"type\": \"text\"\\\n            }\\\n        ]\n    }\n\n    # 2. Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # 3. Set up your crawler config (if needed)\n    config = CrawlerRunConfig(\n        # e.g., pass js_code or wait_for if the page is dynamic\n        # wait_for=\"css:.crypto-row:nth-child(20)\"\n        cache_mode = CacheMode.BYPASS,\n        extraction_strategy=extraction_strategy,\n    )\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # 4. Run the crawl and extraction\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        # 5. Parse the extracted JSON\n        data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(data)} coin entries\")\n        print(json.dumps(data[0], indent=2) if data else \"No data found\")\n\nasyncio.run(extract_crypto_prices())\nCopy\n```\n\n**Highlights**:\n\n- **`baseSelector`**: Tells us where each \"item\" (crypto row) is.\n- **`fields`**: Two fields ( `coin_name`, `price`) using simple CSS selectors.\n- Each field defines a **`type`** (e.g., `text`, `attribute`, `html`, `regex`, etc.).\n\nNo LLM is needed, and the performance is **near-instant** for hundreds or thousands of items.\n\n* * *\n\n### **XPath Example with `raw://` HTML**\n\nBelow is a short example demonstrating **XPath** extraction plus the **`raw://`** scheme. We'll pass a **dummy HTML** directly (no network request) and define the extraction strategy in `CrawlerRunConfig`.\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import JsonXPathExtractionStrategy\n\nasync def extract_crypto_prices_xpath():\n    # 1. Minimal dummy HTML with some repeating rows\n    dummy_html = \"\"\"\n    <html>\n      <body>\n        <div class='crypto-row'>\n          <h2 class='coin-name'>Bitcoin</h2>\n          <span class='coin-price'>$28,000</span>\n        </div>\n        <div class='crypto-row'>\n          <h2 class='coin-name'>Ethereum</h2>\n          <span class='coin-price'>$1,800</span>\n        </div>\n      </body>\n    </html>\n    \"\"\"\n\n    # 2. Define the JSON schema (XPath version)\n    schema = {\n        \"name\": \"Crypto Prices via XPath\",\n        \"baseSelector\": \"//div[@class='crypto-row']\",\n        \"fields\": [\\\n            {\\\n                \"name\": \"coin_name\",\\\n                \"selector\": \".//h2[@class='coin-name']\",\\\n                \"type\": \"text\"\\\n            },\\\n            {\\\n                \"name\": \"price\",\\\n                \"selector\": \".//span[@class='coin-price']\",\\\n                \"type\": \"text\"\\\n            }\\\n        ]\n    }\n\n    # 3. Place the strategy in the CrawlerRunConfig\n    config = CrawlerRunConfig(\n        extraction_strategy=JsonXPathExtractionStrategy(schema, verbose=True)\n    )\n\n    # 4. Use raw:// scheme to pass dummy_html directly\n    raw_url = f\"raw://{dummy_html}\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=raw_url,\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(data)} coin rows\")\n        if data:\n            print(\"First item:\", data[0])\n\nasyncio.run(extract_crypto_prices_xpath())\nCopy\n```\n\n**Key Points**:\n\n1. **`JsonXPathExtractionStrategy`** is used instead of `JsonCssExtractionStrategy`.\n2. **`baseSelector`** and each field's `\"selector\"` use **XPath** instead of CSS.\n3. **`raw://`** lets us pass `dummy_html` with no real network request‚Äîhandy for local testing.\n4. Everything (including the extraction strategy) is in **`CrawlerRunConfig`**.\n\nThat's how you keep the config self-contained, illustrate **XPath** usage, and demonstrate the **raw** scheme for direct HTML input‚Äîall while avoiding the old approach of passing `extraction_strategy` directly to `arun()`.\n\n* * *\n\n## 3\\. Advanced Schema & Nested Structures\n\nReal sites often have **nested** or repeated data‚Äîlike categories containing products, which themselves have a list of reviews or features. For that, we can define **nested** or **list** (and even **nested\\_list**) fields.\n\n### Sample E-Commerce HTML\n\nWe have a **sample e-commerce** HTML file on GitHub (example):\n\n```bash\nhttps://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\nCopy\n```\n\nThis snippet includes categories, products, features, reviews, and related items. Let's see how to define a schema that fully captures that structure **without LLM**.\n\n```graphql\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    # (1) We can define optional baseFields if we want to extract attributes\n    # from the category container\n    \"baseFields\": [\\\n        {\"name\": \"data_cat_id\", \"type\": \"attribute\", \"attribute\": \"data-cat-id\"},\\\n    ],\n    \"fields\": [\\\n        {\\\n            \"name\": \"category_name\",\\\n            \"selector\": \"h2.category-name\",\\\n            \"type\": \"text\"\\\n        },\\\n        {\\\n            \"name\": \"products\",\\\n            \"selector\": \"div.product\",\\\n            \"type\": \"nested_list\",    # repeated sub-objects\\\n            \"fields\": [\\\n                {\\\n                    \"name\": \"name\",\\\n                    \"selector\": \"h3.product-name\",\\\n                    \"type\": \"text\"\\\n                },\\\n                {\\\n                    \"name\": \"price\",\\\n                    \"selector\": \"p.product-price\",\\\n                    \"type\": \"text\"\\\n                },\\\n                {\\\n                    \"name\": \"details\",\\\n                    \"selector\": \"div.product-details\",\\\n                    \"type\": \"nested\",  # single sub-object\\\n                    \"fields\": [\\\n                        {\\\n                            \"name\": \"brand\",\\\n                            \"selector\": \"span.brand\",\\\n                            \"type\": \"text\"\\\n                        },\\\n                        {\\\n                            \"name\": \"model\",\\\n                            \"selector\": \"span.model\",\\\n                            \"type\": \"text\"\\\n                        }\\\n                    ]\\\n                },\\\n                {\\\n                    \"name\": \"features\",\\\n                    \"selector\": \"ul.product-features li\",\\\n                    \"type\": \"list\",\\\n                    \"fields\": [\\\n                        {\"name\": \"feature\", \"type\": \"text\"}\\\n                    ]\\\n                },\\\n                {\\\n                    \"name\": \"reviews\",\\\n                    \"selector\": \"div.review\",\\\n                    \"type\": \"nested_list\",\\\n                    \"fields\": [\\\n                        {\\\n                            \"name\": \"reviewer\",\\\n                            \"selector\": \"span.reviewer\",\\\n                            \"type\": \"text\"\\\n                        },\\\n                        {\\\n                            \"name\": \"rating\",\\\n                            \"selector\": \"span.rating\",\\\n                            \"type\": \"text\"\\\n                        },\\\n                        {\\\n                            \"name\": \"comment\",\\\n                            \"selector\": \"p.review-text\",\\\n                            \"type\": \"text\"\\\n                        }\\\n                    ]\\\n                },\\\n                {\\\n                    \"name\": \"related_products\",\\\n                    \"selector\": \"ul.related-products li\",\\\n                    \"type\": \"list\",\\\n                    \"fields\": [\\\n                        {\\\n                            \"name\": \"name\",\\\n                            \"selector\": \"span.related-name\",\\\n                            \"type\": \"text\"\\\n                        },\\\n                        {\\\n                            \"name\": \"price\",\\\n                            \"selector\": \"span.related-price\",\\\n                            \"type\": \"text\"\\\n                        }\\\n                    ]\\\n                }\\\n            ]\\\n        }\\\n    ]\n}\nCopy\n```\n\nKey Takeaways:\n\n- **Nested vs. List**:\n- **`type: \"nested\"`** means a **single** sub-object (like `details`).\n- **`type: \"list\"`** means multiple items that are **simple** dictionaries or single text fields.\n- **`type: \"nested_list\"`** means repeated **complex** objects (like `products` or `reviews`).\n- **Base Fields**: We can extract **attributes** from the container element via `\"baseFields\"`. For instance, `\"data_cat_id\"` might be `data-cat-id=\"elect123\"`.\n- **Transforms**: We can also define a `transform` if we want to lower/upper case, strip whitespace, or even run a custom function.\n\n### Running the Extraction\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\necommerce_schema = {\n    # ... the advanced schema from above ...\n}\n\nasync def extract_ecommerce_data():\n    strategy = JsonCssExtractionStrategy(ecommerce_schema, verbose=True)\n\n    config = CrawlerRunConfig()\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=strategy,\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        # Parse the JSON output\n        data = json.loads(result.extracted_content)\n        print(json.dumps(data, indent=2) if data else \"No data found.\")\n\nasyncio.run(extract_ecommerce_data())\nCopy\n```\n\nIf all goes well, you get a **structured** JSON array with each \"category,\" containing an array of `products`. Each product includes `details`, `features`, `reviews`, etc. All of that **without** an LLM.\n\n* * *\n\n## 4\\. RegexExtractionStrategy - Fast Pattern-Based Extraction\n\nCrawl4AI now offers a powerful new zero-LLM extraction strategy: `RegexExtractionStrategy`. This strategy provides lightning-fast extraction of common data types like emails, phone numbers, URLs, dates, and more using pre-compiled regular expressions.\n\n### Key Features\n\n- **Zero LLM Dependency**: Extracts data without any AI model calls\n- **Blazing Fast**: Uses pre-compiled regex patterns for maximum performance\n- **Built-in Patterns**: Includes ready-to-use patterns for common data types\n- **Custom Patterns**: Add your own regex patterns for domain-specific extraction\n- **LLM-Assisted Pattern Generation**: Optionally use an LLM once to generate optimized patterns, then reuse them without further LLM calls\n\n### Simple Example: Extracting Common Entities\n\nThe easiest way to start is by using the built-in pattern catalog:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    CrawlerRunConfig,\n    RegexExtractionStrategy\n)\n\nasync def extract_with_regex():\n    # Create a strategy using built-in patterns for URLs and currencies\n    strategy = RegexExtractionStrategy(\n        pattern = RegexExtractionStrategy.Url | RegexExtractionStrategy.Currency\n    )\n\n    config = CrawlerRunConfig(extraction_strategy=strategy)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=config\n        )\n\n        if result.success:\n            data = json.loads(result.extracted_content)\n            for item in data[:5]:  # Show first 5 matches\n                print(f\"{item['label']}: {item['value']}\")\n            print(f\"Total matches: {len(data)}\")\n\nasyncio.run(extract_with_regex())\nCopy\n```\n\n### Available Built-in Patterns\n\n`RegexExtractionStrategy` provides these common patterns as IntFlag attributes for easy combining:\n\n```makefile\n# Use individual patterns\nstrategy = RegexExtractionStrategy(pattern=RegexExtractionStrategy.Email)\n\n# Combine multiple patterns\nstrategy = RegexExtractionStrategy(\n    pattern = (\n        RegexExtractionStrategy.Email |\n        RegexExtractionStrategy.PhoneUS |\n        RegexExtractionStrategy.Url\n    )\n)\n\n# Use all available patterns\nstrategy = RegexExtractionStrategy(pattern=RegexExtractionStrategy.All)\nCopy\n```\n\nAvailable patterns include:\n\\- `Email` \\- Email addresses\n\\- `PhoneIntl` \\- International phone numbers\n\\- `PhoneUS` \\- US-format phone numbers\n\\- `Url` \\- HTTP/HTTPS URLs\n\\- `IPv4` \\- IPv4 addresses\n\\- `IPv6` \\- IPv6 addresses\n\\- `Uuid` \\- UUIDs\n\\- `Currency` \\- Currency values (USD, EUR, etc.)\n\\- `Percentage` \\- Percentage values\n\\- `Number` \\- Numeric values\n\\- `DateIso` \\- ISO format dates\n\\- `DateUS` \\- US format dates\n\\- `Time24h` \\- 24-hour format times\n\\- `PostalUS` \\- US postal codes\n\\- `PostalUK` \\- UK postal codes\n\\- `HexColor` \\- HTML hex color codes\n\\- `TwitterHandle` \\- Twitter handles\n\\- `Hashtag` \\- Hashtags\n\\- `MacAddr` \\- MAC addresses\n\\- `Iban` \\- International bank account numbers\n\\- `CreditCard` \\- Credit card numbers\n\n### Custom Pattern Example\n\nFor more targeted extraction, you can provide custom patterns:\n\n```python\nimport json\nimport asyncio\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    CrawlerRunConfig,\n    RegexExtractionStrategy\n)\n\nasync def extract_prices():\n    # Define a custom pattern for US Dollar prices\n    price_pattern = {\"usd_price\": r\"\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\"}\n\n    # Create strategy with custom pattern\n    strategy = RegexExtractionStrategy(custom=price_pattern)\n    config = CrawlerRunConfig(extraction_strategy=strategy)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.example.com/products\",\n            config=config\n        )\n\n        if result.success:\n            data = json.loads(result.extracted_content)\n            for item in data:\n                print(f\"Found price: {item['value']}\")\n\nasyncio.run(extract_prices())\nCopy\n```\n\n### LLM-Assisted Pattern Generation\n\nFor complex or site-specific patterns, you can use an LLM once to generate an optimized pattern, then save and reuse it without further LLM calls:\n\n```python\nimport json\nimport asyncio\nfrom pathlib import Path\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    CrawlerRunConfig,\n    RegexExtractionStrategy,\n    LLMConfig\n)\n\nasync def extract_with_generated_pattern():\n    cache_dir = Path(\"./pattern_cache\")\n    cache_dir.mkdir(exist_ok=True)\n    pattern_file = cache_dir / \"price_pattern.json\"\n\n    # 1. Generate or load pattern\n    if pattern_file.exists():\n        pattern = json.load(pattern_file.open())\n        print(f\"Using cached pattern: {pattern}\")\n    else:\n        print(\"Generating pattern via LLM...\")\n\n        # Configure LLM\n        llm_config = LLMConfig(\n            provider=\"openai/gpt-4o-mini\",\n            api_token=\"env:OPENAI_API_KEY\",\n        )\n\n        # Get sample HTML for context\n        async with AsyncWebCrawler() as crawler:\n            result = await crawler.arun(\"https://example.com/products\")\n            html = result.fit_html\n\n        # Generate pattern (one-time LLM usage)\n        pattern = RegexExtractionStrategy.generate_pattern(\n            label=\"price\",\n            html=html,\n            query=\"Product prices in USD format\",\n            llm_config=llm_config,\n        )\n\n        # Cache pattern for future use\n        json.dump(pattern, pattern_file.open(\"w\"), indent=2)\n\n    # 2. Use pattern for extraction (no LLM calls)\n    strategy = RegexExtractionStrategy(custom=pattern)\n    config = CrawlerRunConfig(extraction_strategy=strategy)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/products\",\n            config=config\n        )\n\n        if result.success:\n            data = json.loads(result.extracted_content)\n            for item in data[:10]:\n                print(f\"Extracted: {item['value']}\")\n            print(f\"Total matches: {len(data)}\")\n\nasyncio.run(extract_with_generated_pattern())\nCopy\n```\n\nThis pattern allows you to:\n1\\. Use an LLM once to generate a highly optimized regex for your specific site\n2\\. Save the pattern to disk for reuse\n3\\. Extract data using only regex (no further LLM calls) in production\n\n### Extraction Results Format\n\nThe `RegexExtractionStrategy` returns results in a consistent format:\n\n```json\n[\\\n  {\\\n    \"url\": \"https://example.com\",\\\n    \"label\": \"email\",\\\n    \"value\": \"contact@example.com\",\\\n    \"span\": [145, 163]\\\n  },\\\n  {\\\n    \"url\": \"https://example.com\",\\\n    \"label\": \"url\",\\\n    \"value\": \"https://support.example.com\",\\\n    \"span\": [210, 235]\\\n  }\\\n]\nCopy\n```\n\nEach match includes:\n\\- `url`: The source URL\n\\- `label`: The pattern name that matched (e.g., \"email\", \"phone\\_us\")\n\\- `value`: The extracted text\n\\- `span`: The start and end positions in the source content\n\n* * *\n\n## 5\\. Why \"No LLM\" Is Often Better\n\n1. **Zero Hallucination**: Pattern-based extraction doesn't guess text. It either finds it or not.\n2. **Guaranteed Structure**: The same schema or regex yields consistent JSON across many pages, so your downstream pipeline can rely on stable keys.\n3. **Speed**: LLM-based extraction can be 10‚Äì1000x slower for large-scale crawling.\n4. **Scalable**: Adding or updating a field is a matter of adjusting the schema or regex, not re-tuning a model.\n\n**When might you consider an LLM?** Possibly if the site is extremely unstructured or you want AI summarization. But always try a schema or regex approach first for repeated or consistent data patterns.\n\n* * *\n\n## 6\\. Base Element Attributes & Additional Fields\n\nIt's easy to **extract attributes** (like `href`, `src`, or `data-xxx`) from your base or nested elements using:\n\n```json\n{\n  \"name\": \"href\",\n  \"type\": \"attribute\",\n  \"attribute\": \"href\",\n  \"default\": null\n}\nCopy\n```\n\nYou can define them in **`baseFields`** (extracted from the main container element) or in each field's sub-lists. This is especially helpful if you need an item's link or ID stored in the parent `<div>`.\n\n* * *\n\n## 7\\. Putting It All Together: Larger Example\n\nConsider a blog site. We have a schema that extracts the **URL** from each post card (via `baseFields` with an `\"attribute\": \"href\"`), plus the title, date, summary, and author:\n\n```graphql\nschema = {\n  \"name\": \"Blog Posts\",\n  \"baseSelector\": \"a.blog-post-card\",\n  \"baseFields\": [\\\n    {\"name\": \"post_url\", \"type\": \"attribute\", \"attribute\": \"href\"}\\\n  ],\n  \"fields\": [\\\n    {\"name\": \"title\", \"selector\": \"h2.post-title\", \"type\": \"text\", \"default\": \"No Title\"},\\\n    {\"name\": \"date\", \"selector\": \"time.post-date\", \"type\": \"text\", \"default\": \"\"},\\\n    {\"name\": \"summary\", \"selector\": \"p.post-summary\", \"type\": \"text\", \"default\": \"\"},\\\n    {\"name\": \"author\", \"selector\": \"span.post-author\", \"type\": \"text\", \"default\": \"\"}\\\n  ]\n}\nCopy\n```\n\nThen run with `JsonCssExtractionStrategy(schema)` to get an array of blog post objects, each with `\"post_url\"`, `\"title\"`, `\"date\"`, `\"summary\"`, `\"author\"`.\n\n* * *\n\n## 8\\. Tips & Best Practices\n\n1. **Inspect the DOM** in Chrome DevTools or Firefox's Inspector to find stable selectors.\n2. **Start Simple**: Verify you can extract a single field. Then add complexity like nested objects or lists.\n3. **Test** your schema on partial HTML or a test page before a big crawl.\n4. **Combine with JS Execution** if the site loads content dynamically. You can pass `js_code` or `wait_for` in `CrawlerRunConfig`.\n5. **Look at Logs** when `verbose=True`: if your selectors are off or your schema is malformed, it'll often show warnings.\n6. **Use baseFields** if you need attributes from the container element (e.g., `href`, `data-id`), especially for the \"parent\" item.\n7. **Performance**: For large pages, make sure your selectors are as narrow as possible.\n8. **Consider Using Regex First**: For simple data types like emails, URLs, and dates, `RegexExtractionStrategy` is often the fastest approach.\n\n* * *\n\n## 9\\. Schema Generation Utility\n\nWhile manually crafting schemas is powerful and precise, Crawl4AI now offers a convenient utility to **automatically generate** extraction schemas using LLM. This is particularly useful when:\n\n1. You're dealing with a new website structure and want a quick starting point\n2. You need to extract complex nested data structures\n3. You want to avoid the learning curve of CSS/XPath selector syntax\n\n### Using the Schema Generator\n\nThe schema generator is available as a static method on both `JsonCssExtractionStrategy` and `JsonXPathExtractionStrategy`. You can choose between OpenAI's GPT-4 or the open-source Ollama for schema generation:\n\n```python\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy, JsonXPathExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Sample HTML with product information\nhtml = \"\"\"\n<div class=\"product-card\">\n    <h2 class=\"title\">Gaming Laptop</h2>\n    <div class=\"price\">$999.99</div>\n    <div class=\"specs\">\n        <ul>\n            <li>16GB RAM</li>\n            <li>1TB SSD</li>\n        </ul>\n    </div>\n</div>\n\"\"\"\n\n# Option 1: Using OpenAI (requires API token)\ncss_schema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    schema_type=\"css\",\n    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\")\n)\n\n# Option 2: Using Ollama (open source, no token needed)\nxpath_schema = JsonXPathExtractionStrategy.generate_schema(\n    html,\n    schema_type=\"xpath\",\n    llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None)  # Not needed for Ollama\n)\n\n# Use the generated schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(css_schema)\nCopy\n```\n\n### LLM Provider Options\n\n01. **OpenAI GPT-4 ( `openai/gpt4o`)**\n02. Default provider\n03. Requires an API token\n04. Generally provides more accurate schemas\n05. Set via environment variable: `OPENAI_API_KEY`\n\n06. **Ollama ( `ollama/llama3.3`)**\n\n07. Open source alternative\n08. No API token required\n09. Self-hosted option\n10. Good for development and testing\n\n### Benefits of Schema Generation\n\n1. **One-Time Cost**: While schema generation uses LLM, it's a one-time cost. The generated schema can be reused for unlimited extractions without further LLM calls.\n2. **Smart Pattern Recognition**: The LLM analyzes the HTML structure and identifies common patterns, often producing more robust selectors than manual attempts.\n3. **Automatic Nesting**: Complex nested structures are automatically detected and properly represented in the schema.\n4. **Learning Tool**: The generated schemas serve as excellent examples for learning how to write your own schemas.\n\n### Best Practices\n\n1. **Review Generated Schemas**: While the generator is smart, always review and test the generated schema before using it in production.\n2. **Provide Representative HTML**: The better your sample HTML represents the overall structure, the more accurate the generated schema will be.\n3. **Consider Both CSS and XPath**: Try both schema types and choose the one that works best for your specific case.\n4. **Cache Generated Schemas**: Since generation uses LLM, save successful schemas for reuse.\n5. **API Token Security**: Never hardcode API tokens. Use environment variables or secure configuration management.\n6. **Choose Provider Wisely**:\n7. Use OpenAI for production-quality schemas\n8. Use Ollama for development, testing, or when you need a self-hosted solution\n\n* * *\n\n## 10\\. Conclusion\n\nWith Crawl4AI's LLM-free extraction strategies - `JsonCssExtractionStrategy`, `JsonXPathExtractionStrategy`, and now `RegexExtractionStrategy` \\- you can build powerful pipelines that:\n\n- Scrape any consistent site for structured data.\n- Support nested objects, repeating lists, or pattern-based extraction.\n- Scale to thousands of pages quickly and reliably.\n\n**Choosing the Right Strategy**:\n\n- Use **`RegexExtractionStrategy`** for fast extraction of common data types like emails, phones, URLs, dates, etc.\n- Use **`JsonCssExtractionStrategy`** or **`JsonXPathExtractionStrategy`** for structured data with clear HTML patterns\n- If you need both: first extract structured data with JSON strategies, then use regex on specific fields\n\n**Remember**: For repeated, structured data, you don't need to pay for or wait on an LLM. Well-crafted schemas and regex patterns get you the data faster, cleaner, and cheaper‚Äî **the real power** of Crawl4AI.\n\n**Last Updated**: 2025-05-02\n\n* * *\n\nThat's it for **Extracting JSON (No LLM)**! You've seen how schema-based approaches (either CSS or XPath) and regex patterns can handle everything from simple lists to deeply nested product catalogs‚Äîinstantly, with minimal overhead. Enjoy building robust scrapers that produce consistent, structured JSON for your data pipelines!\n\n* * *\n\n>\nFeedback\n\n[Ask AI](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n\n<hr class='page-separator'>\n\n[unclecode/crawl4ai 43.9k 4.1k](https://github.com/unclecode/crawl4ai)\n\n# Fit Markdown with Pruning & BM25\n\n**Fit Markdown** is a specialized **filtered** version of your page‚Äôs markdown, focusing on the most relevant content. By default, Crawl4AI converts the entire HTML into a broad **raw\\_markdown**. With fit markdown, we apply a **content filter** algorithm (e.g., **Pruning** or **BM25**) to remove or rank low-value sections‚Äîsuch as repetitive sidebars, shallow text blocks, or irrelevancies‚Äîleaving a concise textual ‚Äúcore.‚Äù\n\n* * *\n\n## 1\\. How ‚ÄúFit Markdown‚Äù Works\n\n### 1.1 The `content_filter`\n\nIn **`CrawlerRunConfig`**, you can specify a **`content_filter`** to shape how content is pruned or ranked before final markdown generation. A filter‚Äôs logic is applied **before** or **during** the HTML‚ÜíMarkdown process, producing:\n\n- **`result.markdown.raw_markdown`** (unfiltered)\n- **`result.markdown.fit_markdown`** (filtered or ‚Äúfit‚Äù version)\n- **`result.markdown.fit_html`** (the corresponding HTML snippet that produced `fit_markdown`)\n\n### 1.2 Common Filters\n\n1.\\u2000**PruningContentFilter** ‚Äì Scores each node by text density, link density, and tag importance, discarding those below a threshold.\n\n2.\\u2000**BM25ContentFilter** ‚Äì Focuses on textual relevance using BM25 ranking, especially useful if you have a specific user query (e.g., ‚Äúmachine learning‚Äù or ‚Äúfood nutrition‚Äù).\n\n* * *\n\n## 2\\. PruningContentFilter\n\n**Pruning** discards less relevant nodes based on **text density, link density, and tag importance**. It‚Äôs a heuristic-based approach‚Äîif certain sections appear too ‚Äúthin‚Äù or too ‚Äúspammy,‚Äù they‚Äôre pruned.\n\n### 2.1 Usage Example\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    # Step 1: Create a pruning filter\n    prune_filter = PruningContentFilter(\n        # Lower ‚Üí more content retained, higher ‚Üí more content pruned\n        threshold=0.45,\n        # \"fixed\" or \"dynamic\"\n        threshold_type=\"dynamic\",\n        # Ignore nodes with <5 words\n        min_word_threshold=5\n    )\n\n    # Step 2: Insert it into a Markdown Generator\n    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\n\n    # Step 3: Pass it to CrawlerRunConfig\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=config\n        )\n\n        if result.success:\n            # 'fit_markdown' is your pruned content, focusing on \"denser\" text\n            print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n            print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### 2.2 Key Parameters\n\n- **`min_word_threshold`** (int): If a block has fewer words than this, it‚Äôs pruned.\n- **`threshold_type`** (str):\n- `\"fixed\"` ‚Üí each node must exceed `threshold` (0‚Äì1).\n- `\"dynamic\"` ‚Üí node scoring adjusts according to tag type, text/link density, etc.\n- **`threshold`** (float, default ~0.48): The base or ‚Äúanchor‚Äù cutoff.\n\n**Algorithmic Factors**:\n\n- **Text density** ‚Äì Encourages blocks that have a higher ratio of text to overall content.\n- **Link density** ‚Äì Penalizes sections that are mostly links.\n- **Tag importance** ‚Äì e.g., an `<article>` or `<p>` might be more important than a `<div>`.\n- **Structural context** ‚Äì If a node is deeply nested or in a suspected sidebar, it might be deprioritized.\n\n* * *\n\n## 3\\. BM25ContentFilter\n\n**BM25** is a classical text ranking algorithm often used in search engines. If you have a **user query** or rely on page metadata to derive a query, BM25 can identify which text chunks best match that query.\n\n### 3.1 Usage Example\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    # 1) A BM25 filter with a user query\n    bm25_filter = BM25ContentFilter(\n        user_query=\"startup fundraising tips\",\n        # Adjust for stricter or looser results\n        bm25_threshold=1.2\n    )\n\n    # 2) Insert into a Markdown Generator\n    md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)\n\n    # 3) Pass to crawler config\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=config\n        )\n        if result.success:\n            print(\"Fit Markdown (BM25 query-based):\")\n            print(result.markdown.fit_markdown)\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### 3.2 Parameters\n\n- **`user_query`** (str, optional): E.g. `\"machine learning\"`. If blank, the filter tries to glean a query from page metadata.\n- **`bm25_threshold`** (float, default 1.0):\n- Higher ‚Üí fewer chunks but more relevant.\n- Lower ‚Üí more inclusive.\n\n> In more advanced scenarios, you might see parameters like `use_stemming`, `case_sensitive`, or `priority_tags` to refine how text is tokenized or weighted.\n\n* * *\n\n## 4\\. Accessing the ‚ÄúFit‚Äù Output\n\nAfter the crawl, your ‚Äúfit‚Äù content is found in **`result.markdown.fit_markdown`**.\n\n```ini\nfit_md = result.markdown.fit_markdown\nfit_html = result.markdown.fit_html\nCopy\n```\n\nIf the content filter is **BM25**, you might see additional logic or references in `fit_markdown` that highlight relevant segments. If it‚Äôs **Pruning**, the text is typically well-cleaned but not necessarily matched to a query.\n\n* * *\n\n## 5\\. Code Patterns Recap\n\n### 5.1 Pruning\n\n```makefile\nprune_filter = PruningContentFilter(\n    threshold=0.5,\n    threshold_type=\"fixed\",\n    min_word_threshold=10\n)\nmd_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\nCopy\n```\n\n### 5.2 BM25\n\n```makefile\nbm25_filter = BM25ContentFilter(\n    user_query=\"health benefits fruit\",\n    bm25_threshold=1.2\n)\nmd_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\nCopy\n```\n\n* * *\n\n## 6\\. Combining with ‚Äúword\\_count\\_threshold‚Äù & Exclusions\n\nRemember you can also specify:\n\n```graphql\nconfig = CrawlerRunConfig(\n    word_count_threshold=10,\n    excluded_tags=[\"nav\", \"footer\", \"header\"],\n    exclude_external_links=True,\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=PruningContentFilter(threshold=0.5)\n    )\n)\nCopy\n```\n\nThus, **multi-level** filtering occurs:\n\n1. The crawler‚Äôs `excluded_tags` are removed from the HTML first.\n2. The content filter (Pruning, BM25, or custom) prunes or ranks the remaining text blocks.\n3. The final ‚Äúfit‚Äù content is generated in `result.markdown.fit_markdown`.\n\n* * *\n\n## 7\\. Custom Filters\n\nIf you need a different approach (like a specialized ML model or site-specific heuristics), you can create a new class inheriting from `RelevantContentFilter` and implement `filter_content(html)`. Then inject it into your **markdown generator**:\n\n```python\nfrom crawl4ai.content_filter_strategy import RelevantContentFilter\n\nclass MyCustomFilter(RelevantContentFilter):\n    def filter_content(self, html, min_word_threshold=None):\n        # parse HTML, implement custom logic\n        return [block for block in ... if ... some condition...]\nCopy\n```\n\n**Steps**:\n\n1. Subclass `RelevantContentFilter`.\n2. Implement `filter_content(...)`.\n3. Use it in your `DefaultMarkdownGenerator(content_filter=MyCustomFilter(...))`.\n\n* * *\n\n## 8\\. Final Thoughts\n\n**Fit Markdown** is a crucial feature for:\n\n- **Summaries**: Quickly get the important text from a cluttered page.\n- **Search**: Combine with **BM25** to produce content relevant to a query.\n- **AI Pipelines**: Filter out boilerplate so LLM-based extraction or summarization runs on denser text.\n\n**Key Points**:\n\\- **PruningContentFilter**: Great if you just want the ‚Äúmeatiest‚Äù text without a user query.\n\n\\- **BM25ContentFilter**: Perfect for query-based extraction or searching.\n\n\\- Combine with **`excluded_tags`, `exclude_external_links`, `word_count_threshold`** to refine your final ‚Äúfit‚Äù text.\n\n\\- Fit markdown ends up in **`result.markdown.fit_markdown`**; eventually **`result.markdown.fit_markdown`** in future versions.\n\nWith these tools, you can **zero in** on the text that truly matters, ignoring spammy or boilerplate content, and produce a concise, relevant ‚Äúfit markdown‚Äù for your AI or data pipelines. Happy pruning and searching!\n\n- Last Updated: 2025-01-01\n\n* * *\n\n>\nFeedback\n\n[Ask AI](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n\n<hr class='page-separator'>\n\n[unclecode/crawl4ai 43.9k 4.1k](https://github.com/unclecode/crawl4ai)\n\n# Deep Crawling\n\nOne of Crawl4AI's most powerful features is its ability to perform **configurable deep crawling** that can explore websites beyond a single page. With fine-tuned control over crawl depth, domain boundaries, and content filtering, Crawl4AI gives you the tools to extract precisely the content you need.\n\nIn this tutorial, you'll learn:\n\n1. How to set up a **Basic Deep Crawler** with BFS strategy\n2. Understanding the difference between **streamed and non-streamed** output\n3. Implementing **filters and scorers** to target specific content\n4. Creating **advanced filtering chains** for sophisticated crawls\n5. Using **BestFirstCrawling** for intelligent exploration prioritization\n\n> **Prerequisites**\n>\n> \\- You‚Äôve completed or read [AsyncWebCrawler Basics](https://docs.crawl4ai.com/core/simple-crawling/) to understand how to run a simple crawl.\n>\n> \\- You know how to configure `CrawlerRunConfig`.\n\n* * *\n\n## 1\\. Quick Example\n\nHere's a minimal code snippet that implements a basic deep crawl using the **BFSDeepCrawlStrategy**:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.deep_crawling import BFSDeepCrawlStrategy\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\n\nasync def main():\n    # Configure a 2-level deep crawl\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BFSDeepCrawlStrategy(\n            max_depth=2,\n            include_external=False\n        ),\n        scraping_strategy=LXMLWebScrapingStrategy(),\n        verbose=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        results = await crawler.arun(\"https://example.com\", config=config)\n\n        print(f\"Crawled {len(results)} pages in total\")\n\n        # Access individual results\n        for result in results[:3]:  # Show first 3 results\n            print(f\"URL: {result.url}\")\n            print(f\"Depth: {result.metadata.get('depth', 0)}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**What's happening?**\n\n\\- `BFSDeepCrawlStrategy(max_depth=2, include_external=False)` instructs Crawl4AI to:\n\\- Crawl the starting page (depth 0) plus 2 more levels\n\\- Stay within the same domain (don't follow external links)\n\\- Each result contains metadata like the crawl depth\n\\- Results are returned as a list after all crawling is complete\n\n* * *\n\n## 2\\. Understanding Deep Crawling Strategy Options\n\n### 2.1 BFSDeepCrawlStrategy (Breadth-First Search)\n\nThe **BFSDeepCrawlStrategy** uses a breadth-first approach, exploring all links at one depth before moving deeper:\n\n```python\nfrom crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n\n# Basic configuration\nstrategy = BFSDeepCrawlStrategy(\n    max_depth=2,               # Crawl initial page + 2 levels deep\n    include_external=False,    # Stay within the same domain\n    max_pages=50,              # Maximum number of pages to crawl (optional)\n    score_threshold=0.3,       # Minimum score for URLs to be crawled (optional)\n)\nCopy\n```\n\n**Key parameters:**\n\\- **`max_depth`**: Number of levels to crawl beyond the starting page\n\\- **`include_external`**: Whether to follow links to other domains\n\\- **`max_pages`**: Maximum number of pages to crawl (default: infinite)\n\\- **`score_threshold`**: Minimum score for URLs to be crawled (default: -inf)\n\\- **`filter_chain`**: FilterChain instance for URL filtering\n\\- **`url_scorer`**: Scorer instance for evaluating URLs\n\n### 2.2 DFSDeepCrawlStrategy (Depth-First Search)\n\nThe **DFSDeepCrawlStrategy** uses a depth-first approach, explores as far down a branch as possible before backtracking.\n\n```python\nfrom crawl4ai.deep_crawling import DFSDeepCrawlStrategy\n\n# Basic configuration\nstrategy = DFSDeepCrawlStrategy(\n    max_depth=2,               # Crawl initial page + 2 levels deep\n    include_external=False,    # Stay within the same domain\n    max_pages=30,              # Maximum number of pages to crawl (optional)\n    score_threshold=0.5,       # Minimum score for URLs to be crawled (optional)\n)\nCopy\n```\n\n**Key parameters:**\n\\- **`max_depth`**: Number of levels to crawl beyond the starting page\n\\- **`include_external`**: Whether to follow links to other domains\n\\- **`max_pages`**: Maximum number of pages to crawl (default: infinite)\n\\- **`score_threshold`**: Minimum score for URLs to be crawled (default: -inf)\n\\- **`filter_chain`**: FilterChain instance for URL filtering\n\\- **`url_scorer`**: Scorer instance for evaluating URLs\n\n### 2.3 BestFirstCrawlingStrategy (‚≠êÔ∏è - Recommended Deep crawl strategy)\n\nFor more intelligent crawling, use **BestFirstCrawlingStrategy** with scorers to prioritize the most relevant pages:\n\n```python\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n\n# Create a scorer\nscorer = KeywordRelevanceScorer(\n    keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n    weight=0.7\n)\n\n# Configure the strategy\nstrategy = BestFirstCrawlingStrategy(\n    max_depth=2,\n    include_external=False,\n    url_scorer=scorer,\n    max_pages=25,              # Maximum number of pages to crawl (optional)\n)\nCopy\n```\n\nThis crawling approach:\n\\- Evaluates each discovered URL based on scorer criteria\n\\- Visits higher-scoring pages first\n\\- Helps focus crawl resources on the most relevant content\n\\- Can limit total pages crawled with `max_pages`\n\\- Does not need `score_threshold` as it naturally prioritizes by score\n\n* * *\n\n## 3\\. Streaming vs. Non-Streaming Results\n\nCrawl4AI can return results in two modes:\n\n### 3.1 Non-Streaming Mode (Default)\n\n```csharp\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),\n    stream=False  # Default behavior\n)\n\nasync with AsyncWebCrawler() as crawler:\n    # Wait for ALL results to be collected before returning\n    results = await crawler.arun(\"https://example.com\", config=config)\n\n    for result in results:\n        process_result(result)\nCopy\n```\n\n**When to use non-streaming mode:**\n\\- You need the complete dataset before processing\n\\- You're performing batch operations on all results together\n\\- Crawl time isn't a critical factor\n\n### 3.2 Streaming Mode\n\n```python\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),\n    stream=True  # Enable streaming\n)\n\nasync with AsyncWebCrawler() as crawler:\n    # Returns an async iterator\n    async for result in await crawler.arun(\"https://example.com\", config=config):\n        # Process each result as it becomes available\n        process_result(result)\nCopy\n```\n\n**Benefits of streaming mode:**\n\\- Process results immediately as they're discovered\n\\- Start working with early results while crawling continues\n\\- Better for real-time applications or progressive display\n\\- Reduces memory pressure when handling many pages\n\n* * *\n\n## 4\\. Filtering Content with Filter Chains\n\nFilters help you narrow down which pages to crawl. Combine multiple filters using **FilterChain** for powerful targeting.\n\n### 4.1 Basic URL Pattern Filter\n\n```sql\nfrom crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter\n\n# Only follow URLs containing \"blog\" or \"docs\"\nurl_filter = URLPatternFilter(patterns=[\"*blog*\", \"*docs*\"])\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([url_filter])\n    )\n)\nCopy\n```\n\n### 4.2 Combining Multiple Filters\n\n```makefile\nfrom crawl4ai.deep_crawling.filters import (\n    FilterChain,\n    URLPatternFilter,\n    DomainFilter,\n    ContentTypeFilter\n)\n\n# Create a chain of filters\nfilter_chain = FilterChain([\\\n    # Only follow URLs with specific patterns\\\n    URLPatternFilter(patterns=[\"*guide*\", \"*tutorial*\"]),\\\n\\\n    # Only crawl specific domains\\\n    DomainFilter(\\\n        allowed_domains=[\"docs.example.com\"],\\\n        blocked_domains=[\"old.docs.example.com\"]\n    ),\\\n\\\n    # Only include specific content types\\\n    ContentTypeFilter(allowed_types=[\"text/html\"])\\\n])\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=2,\n        filter_chain=filter_chain\n    )\n)\nCopy\n```\n\n### 4.3 Available Filter Types\n\nCrawl4AI includes several specialized filters:\n\n- **`URLPatternFilter`**: Matches URL patterns using wildcard syntax\n- **`DomainFilter`**: Controls which domains to include or exclude\n- **`ContentTypeFilter`**: Filters based on HTTP Content-Type\n- **`ContentRelevanceFilter`**: Uses similarity to a text query\n- **`SEOFilter`**: Evaluates SEO elements (meta tags, headers, etc.)\n\n* * *\n\n## 5\\. Using Scorers for Prioritized Crawling\n\nScorers assign priority values to discovered URLs, helping the crawler focus on the most relevant content first.\n\n### 5.1 KeywordRelevanceScorer\n\n```python\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\n\n# Create a keyword relevance scorer\nkeyword_scorer = KeywordRelevanceScorer(\n    keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n    weight=0.7  # Importance of this scorer (0.0 to 1.0)\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BestFirstCrawlingStrategy(\n        max_depth=2,\n        url_scorer=keyword_scorer\n    ),\n    stream=True  # Recommended with BestFirstCrawling\n)\n\n# Results will come in order of relevance score\nasync with AsyncWebCrawler() as crawler:\n    async for result in await crawler.arun(\"https://example.com\", config=config):\n        score = result.metadata.get(\"score\", 0)\n        print(f\"Score: {score:.2f} | {result.url}\")\nCopy\n```\n\n**How scorers work:**\n\\- Evaluate each discovered URL before crawling\n\\- Calculate relevance based on various signals\n\\- Help the crawler make intelligent choices about traversal order\n\n* * *\n\n## 6\\. Advanced Filtering Techniques\n\n### 6.1 SEO Filter for Quality Assessment\n\nThe **SEOFilter** helps you identify pages with strong SEO characteristics:\n\n```makefile\nfrom crawl4ai.deep_crawling.filters import FilterChain, SEOFilter\n\n# Create an SEO filter that looks for specific keywords in page metadata\nseo_filter = SEOFilter(\n    threshold=0.5,  # Minimum score (0.0 to 1.0)\n    keywords=[\"tutorial\", \"guide\", \"documentation\"]\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([seo_filter])\n    )\n)\nCopy\n```\n\n### 6.2 Content Relevance Filter\n\nThe **ContentRelevanceFilter** analyzes the actual content of pages:\n\n```makefile\nfrom crawl4ai.deep_crawling.filters import FilterChain, ContentRelevanceFilter\n\n# Create a content relevance filter\nrelevance_filter = ContentRelevanceFilter(\n    query=\"Web crawling and data extraction with Python\",\n    threshold=0.7  # Minimum similarity score (0.0 to 1.0)\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([relevance_filter])\n    )\n)\nCopy\n```\n\nThis filter:\n\\- Measures semantic similarity between query and page content\n\\- It's a BM25-based relevance filter using head section content\n\n* * *\n\n## 7\\. Building a Complete Advanced Crawler\n\nThis example combines multiple techniques for a sophisticated crawl:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\nfrom crawl4ai.deep_crawling.filters import (\n    FilterChain,\n    DomainFilter,\n    URLPatternFilter,\n    ContentTypeFilter\n)\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n\nasync def run_advanced_crawler():\n    # Create a sophisticated filter chain\n    filter_chain = FilterChain([\\\n        # Domain boundaries\\\n        DomainFilter(\\\n            allowed_domains=[\"docs.example.com\"],\\\n            blocked_domains=[\"old.docs.example.com\"]\n        ),\\\n\\\n        # URL patterns to include\\\n        URLPatternFilter(patterns=[\"*guide*\", \"*tutorial*\", \"*blog*\"]),\\\n\\\n        # Content type filtering\\\n        ContentTypeFilter(allowed_types=[\"text/html\"])\\\n    ])\n\n    # Create a relevance scorer\n    keyword_scorer = KeywordRelevanceScorer(\n        keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n        weight=0.7\n    )\n\n    # Set up the configuration\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BestFirstCrawlingStrategy(\n            max_depth=2,\n            include_external=False,\n            filter_chain=filter_chain,\n            url_scorer=keyword_scorer\n        ),\n        scraping_strategy=LXMLWebScrapingStrategy(),\n        stream=True,\n        verbose=True\n    )\n\n    # Execute the crawl\n    results = []\n    async with AsyncWebCrawler() as crawler:\n        async for result in await crawler.arun(\"https://docs.example.com\", config=config):\n            results.append(result)\n            score = result.metadata.get(\"score\", 0)\n            depth = result.metadata.get(\"depth\", 0)\n            print(f\"Depth: {depth} | Score: {score:.2f} | {result.url}\")\n\n    # Analyze the results\n    print(f\"Crawled {len(results)} high-value pages\")\n    print(f\"Average score: {sum(r.metadata.get('score', 0) for r in results) / len(results):.2f}\")\n\n    # Group by depth\n    depth_counts = {}\n    for result in results:\n        depth = result.metadata.get(\"depth\", 0)\n        depth_counts[depth] = depth_counts.get(depth, 0) + 1\n\n    print(\"Pages crawled by depth:\")\n    for depth, count in sorted(depth_counts.items()):\n        print(f\"  Depth {depth}: {count} pages\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_advanced_crawler())\nCopy\n```\n\n* * *\n\n## 8\\. Limiting and Controlling Crawl Size\n\n### 8.1 Using max\\_pages\n\nYou can limit the total number of pages crawled with the `max_pages` parameter:\n\n```makefile\n# Limit to exactly 20 pages regardless of depth\nstrategy = BFSDeepCrawlStrategy(\n    max_depth=3,\n    max_pages=20\n)\nCopy\n```\n\nThis feature is useful for:\n\\- Controlling API costs\n\\- Setting predictable execution times\n\\- Focusing on the most important content\n\\- Testing crawl configurations before full execution\n\n### 8.2 Using score\\_threshold\n\nFor BFS and DFS strategies, you can set a minimum score threshold to only crawl high-quality pages:\n\n```makefile\n# Only follow links with scores above 0.4\nstrategy = DFSDeepCrawlStrategy(\n    max_depth=2,\n    url_scorer=KeywordRelevanceScorer(keywords=[\"api\", \"guide\", \"reference\"]),\n    score_threshold=0.4  # Skip URLs with scores below this value\n)\nCopy\n```\n\nNote that for BestFirstCrawlingStrategy, score\\_threshold is not needed since pages are already processed in order of highest score first.\n\n## 9\\. Common Pitfalls & Tips\n\n1. **Set realistic limits.** Be cautious with `max_depth` values > 3, which can exponentially increase crawl size. Use `max_pages` to set hard limits.\n\n2. **Don't neglect the scoring component.** BestFirstCrawling works best with well-tuned scorers. Experiment with keyword weights for optimal prioritization.\n\n3. **Be a good web citizen.** Respect robots.txt. (disabled by default)\n\n4. **Handle page errors gracefully.** Not all pages will be accessible. Check `result.status` when processing results.\n\n5. **Balance breadth vs. depth.** Choose your strategy wisely - BFS for comprehensive coverage, DFS for deep exploration, BestFirst for focused relevance-based crawling.\n\n* * *\n\n## 10\\. Summary & Next Steps\n\nIn this **Deep Crawling with Crawl4AI** tutorial, you learned to:\n\n- Configure **BFSDeepCrawlStrategy**, **DFSDeepCrawlStrategy**, and **BestFirstCrawlingStrategy**\n- Process results in streaming or non-streaming mode\n- Apply filters to target specific content\n- Use scorers to prioritize the most relevant pages\n- Limit crawls with `max_pages` and `score_threshold` parameters\n- Build a complete advanced crawler with combined techniques\n\nWith these tools, you can efficiently extract structured data from websites at scale, focusing precisely on the content you need for your specific use case.\n\n* * *\n\n>\nFeedback\n\n[Ask AI](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")"
  }
]